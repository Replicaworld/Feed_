{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9670d46c-bcf0-4043-9344-53f28e5895b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload \n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3928e-42be-438d-b02e-19db09a32f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bb4013-d9e8-4295-9358-faef0dc68149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2ba1a6-e55e-45ed-bf75-b0558e48505f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kya hai\n",
      "time: 15.7 ms (started: 2023-02-19 07:49:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StructField, StructType, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "print('kya hai')\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, avg\n",
    "from matplotlib.pyplot import scatter\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StructField, StructType, StringType, IntegerType, TimestampType\n",
    "#from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "#from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.sql.functions import countDistinct\n",
    "#from pyspark.ml.linalg import Vectors, SparseVector\n",
    "#from pyspark.ml.clustering import LDA\n",
    "#from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "import datetime\n",
    "from  pyspark.sql.functions import abs\n",
    "from  pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import desc\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from dateutil.parser import parse\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "NIS_DATA_BASE_PATH = \"gs://nis-segment-datasource-v3/processed/\"\n",
    "NIS_OLD_DATA_BASE_PATH = \"gs://nis-localytics-datasource/processed/\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf().setAll([('spark.sql.broadcastTimeout',1000)\n",
    "    # ('spark.driver.memory', '20g'), \\\n",
    "#                            ('spark.broadcast.blockSize', '10m'),\\\n",
    "#                            ('spark.dynamicAllocation.enabled','False'),\\\n",
    "#                            ('spark.executor.instances','9999')\n",
    "# #                            ,(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "                          ])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e086d4-4757-4177-a9da-7454886692d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-11 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def to_disk(a,path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def from_disk(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "def get_path(dates, prefix, padding=None):\n",
    "    dates_ = dates + []\n",
    "    if padding:\n",
    "        st_date, ed_date = sorted(dates)[0], sorted(dates)[-1]\n",
    "        for i in range(1, 4):\n",
    "            d = (datetime.datetime.strptime(ed_date, date_fmt) + datetime.timedelta(days=i)).strftime(date_fmt)\n",
    "            if d < datetime.datetime.today().strftime(date_fmt):\n",
    "                dates_.append(d)\n",
    "    dates_ = list(set(dates_) - set([\"2019/02/17\", \"2019/02/18\", \"2019/05/28\", \"2019/06/03\", \"2019/07/02\", \"2019/07/03\", \"2019/07/04\", \"2019/11/13\", \"2019/11/14\", \"2020/02/22\", \"2020/03/31\", \"2020/04/16\", \"2020/04/18\", \"2020/05/11\", \"2021/05/13\"]))\n",
    "    paths = []\n",
    "    for date in dates_:\n",
    "        base_path = NIS_DATA_BASE_PATH\n",
    "        if date < \"2018/06/26\":\n",
    "            base_path = NIS_OLD_DATA_BASE_PATH\n",
    "        paths.append(base_path + date + \"/\" + prefix + \"/*.parquet\")\n",
    "    return paths\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "month_fmt = \"%Y/%m\"\n",
    "\n",
    "def millis2date(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(date_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(date_fmt)\n",
    "    except:\n",
    "        return \"1970/01/01\"\n",
    "\n",
    "def millis2month(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(month_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(month_fmt)\n",
    "    except:\n",
    "        return \"1970/01\"\n",
    "\n",
    "millis2date_udf = F.udf(millis2date, StringType())\n",
    "millis2month_udf = F.udf(millis2month, StringType())\n",
    "\n",
    "def divide_maps(d1, d2):\n",
    "    keys = set(d1.keys()).intersection(set(d2.keys()))\n",
    "    res = {}\n",
    "    for k in keys:\n",
    "        res[k] = d1[k] * 1. / (d2[k] + 1e-10)\n",
    "    return res\n",
    "\n",
    "def timediff(y, x, date_fmt=\"%Y/%m/%d\"): \n",
    "    end = datetime.datetime.strptime(y, date_fmt)\n",
    "    start = datetime.datetime.strptime(x, date_fmt)\n",
    "    delta = (end - start).days\n",
    "    return delta\n",
    "\n",
    "def monthdiff(y, x, month_fmt=\"%Y/%m\"): \n",
    "    millis = y - x\n",
    "    delta = millis / (1000 * 3600 * 24 * 30)\n",
    "    return delta\n",
    "\n",
    "timediff_udf = udf(timediff, IntegerType())\n",
    "monthdiff_udf = udf(monthdiff, IntegerType())\n",
    "\n",
    "def filter_platform(data, platform=None):\n",
    "    if platform == \"ANDROID\":\n",
    "        data = data.filter(data.platform == \"ANDROID\")\n",
    "    elif platform == \"IOS\":\n",
    "        data = data.filter(data.platform != \"ANDROID\")\n",
    "    return data\n",
    "\n",
    "def filter_category(data, categories=None):\n",
    "    if categories:\n",
    "        data = data.filter(data.categoryWhenEventHappened.isin(categories))\n",
    "    return data\n",
    "\n",
    "def filter_tenant(data, tenant=None):\n",
    "    if tenant in ['hi', 'HINDI']:\n",
    "        data = data.filter(data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    elif tenant in ['en', 'ENGLISH']:\n",
    "        data = data.filter(~data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_app(data, app_name=None):\n",
    "    if 'appName' in data.columns:\n",
    "        if app_name:\n",
    "            data = data.filter(data.appName == app_name)\n",
    "        else:\n",
    "            data = data.filter((data.appName != \"mini\") & (data.appName != \"crux\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "#### Hash definition\n",
    "def InternalSeqHash(deviceId):\n",
    "    h = 0\n",
    "    for c in list(deviceId):\n",
    "        h = (31*h + ord(c)) & 0xFFFFFFFF\n",
    "    return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000\n",
    "\n",
    "def hashIdentifier(deviceId, M=100):\n",
    "    hId = (abs(InternalSeqHash(deviceId)) % M) + 1\n",
    "    return hId\n",
    "\n",
    "\n",
    "def date_range_generator(start_date,n_days):\n",
    "    date_fmt = \"%Y/%m/%d\"\n",
    "    sign = (-1 if n_days<0 else 1)\n",
    "    n_days = abs(n_days)\n",
    "    st_date = parse(start_date)\n",
    "    return [(st_date + datetime.timedelta(days=sign*i)).strftime(date_fmt) for i in range(n_days)]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.sql.functions import countDistinct\n",
    "#from pyspark.ml.liqnalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.sql.functions import countDistinct\n",
    "#from pyspark.ml.liqnalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Content data for news hash ids\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "def getColl(hosts,db_name,coll_name):\n",
    "    hosts = \",\".join(hosts)\n",
    "    url = \"mongodb://root:superman@\"+hosts+\"/admin\"\n",
    "    client = MongoClient(url,serverSelectionTimeoutMS=8000)\n",
    "    coll = client[db_name][coll_name]\n",
    "    return coll\n",
    "\n",
    "\n",
    "start_dt_time = datetime.datetime(2023, 1, 11)\n",
    "\n",
    "\n",
    "hosts = [\"172.16.11.196\", \"172.16.11.195\", \"172.16.11.194\"]\n",
    "db_name = \"nis-news\"\n",
    "coll_news = getColl(hosts,db_name,'News')\n",
    "\n",
    "\n",
    "#start_dt_time= datetime.datetime.now() - datetime.timedelta(days=163)\n",
    "\n",
    "print(start_dt_time)\n",
    "\n",
    "all_news = coll_news.find({\"createdAt\":{\"$gt\":start_dt_time}},{'_id':1,\"categories\":1,\"createdAt\":1,\"internalCategories\":1,\"title\":1,\"content\":1})\n",
    "\n",
    "#merging content and hash ids\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "#DataFrame[deviceId: string, overallTimeSpent: double, hashId: string, eventTimestamp: bigint]\n",
    "#keep same date for dataframe\n",
    "def data_content(df_,st_date=datetime.datetime(2022, 12, 13)):\n",
    "    def getColl(hosts,db_name,coll_name):\n",
    "        hosts = \",\".join(hosts)\n",
    "        url = \"mongodb://root:superman@\"+hosts+\"/admin\"\n",
    "        client = MongoClient(url,serverSelectionTimeoutMS=8000)\n",
    "        coll = client[db_name][coll_name]\n",
    "        return coll\n",
    "\n",
    "\n",
    "    start_dt_time = st_date\n",
    "\n",
    "\n",
    "    hosts = [\"172.16.11.196\", \"172.16.11.195\", \"172.16.11.194\"]\n",
    "    db_name = \"nis-news\"\n",
    "    coll_news = getColl(hosts,db_name,'News')\n",
    "\n",
    "\n",
    "    #start_dt_time= datetime.datetime.now() - datetime.timedelta(days=163)\n",
    "\n",
    "    print(start_dt_time)\n",
    "\n",
    "    all_news = coll_news.find({\"createdAt\":{\"$gt\":start_dt_time}},{'_id':1,\"categories\":1,\"createdAt\":1,\"internalCategories\":1,\"title\":1,\"content\":1})\n",
    "    \n",
    "    list_cur = list(all_news)\n",
    "    df_content = DataFrame(list_cur)\n",
    "    DF_cont=spark.createDataFrame(df_content[['content','title','_id']])#.cache() \n",
    "    DF_final=df_.join(DF_cont,df_.hashId ==  DF_cont._id,\"inner\").select('deviceId','hashId','overallTimeSpent','content','title')\n",
    "    \n",
    "    return DF_final\n",
    "    \n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords=STOP_WORDS\n",
    "\n",
    "import string\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = tokenization(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "prepr_udf = F.udf(preprocess,ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def merge_list(list1,list2):\n",
    "    lis=list1+list2\n",
    "    return lis\n",
    "merge_udf=F.udf(merge_list,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78407e32-1c39-416f-a963-745a59c7e971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st_date = datetime.datetime(2023, 2, 11)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "n_days = 0\n",
    "\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)).strftime(date_fmt) for i in range(n_days+1)]\n",
    "#print('dates:',dates)\n",
    "\n",
    "\n",
    "#timespent data\n",
    "paths = get_path(dates, 'timeSpentFrontEvents')\n",
    "#paths = get_path(dates, 'otherEvents')\n",
    "data_timespent = (sqlContext.read.parquet(*paths).select(\"deviceId\",\"overallTimeSpent\",\"hashId\",\"eventTimestamp\",\"categoryWhenEventHappened\",\"eventName\")\n",
    ".filter(\"categoryWhenEventHappened='My Feed' and eventName='TimeSpent-Front' and overallTimeSpent<58\")\n",
    ".select(\"deviceId\",\"overallTimeSpent\",\"hashId\",\"eventTimestamp\")\n",
    "                 )\n",
    "\n",
    "st_date = datetime.datetime(2023, 1, 11)\n",
    "n_days = 0\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)).strftime(date_fmt) for i in range(n_days+1)]\n",
    "#print('dates:',dates)\n",
    "\n",
    "\n",
    "#timespent data\n",
    "paths = get_path(dates, 'timeSpentFrontEvents')\n",
    "data_dec = (sqlContext.read.parquet(*paths)\n",
    ".select(\"deviceId\",\"overallTimeSpent\",\"hashId\",\"eventTimestamp\",\"categoryWhenEventHappened\",\"eventName\")\n",
    ".filter(\"categoryWhenEventHappened='My Feed' and eventName='TimeSpent-Front' and overallTimeSpent<58\").select(\"deviceId\")\n",
    "            #major change#\n",
    "            .distinct()\n",
    ")\n",
    "\n",
    "\n",
    "Data_LDA=data_timespent.join(data_dec,['deviceId'],\"inner\")\n",
    "\n",
    "\n",
    "#tmp_loc=\"gs://pvtrough_asia_south1/tmp/gourav_model\"\n",
    "tmp_loc=\"/home/Gourav/Set_B\"\n",
    "(data_content(Data_LDA,st_date=datetime.datetime(2023, 2, 11))\n",
    ".filter(\"title is Not Null and content is Not Null\")\n",
    ".withColumn(\"Preprocess_title\", prepr_udf(\"title\"))\n",
    ".withColumn(\"Preprocess_content\", prepr_udf(\"content\"))\n",
    ".withColumn(\"feature\", merge_udf(\"Preprocess_title\",\"Preprocess_content\"))          \n",
    ".select('deviceId','hashId','overallTimeSpent','feature')          \n",
    "         ).write.parquet(tmp_loc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bcdd92-1e4a-43a0-bea5-979a245a63aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "def getNewsData(d1, d2):\n",
    "    newsMap = getNewsInDates(d1, d2)\n",
    "    hashIdList = list(newsMap.keys())\n",
    "\n",
    "    hashIdsWithFilter = []\n",
    "    for h in hashIdList:\n",
    "        if 'newsLanguage' in newsMap[h] and newsMap[h]['newsLanguage'] == 'english' and newsMap[h]['publishGroupList'][0]['countryCode'] == 'IN':\n",
    "            hashIdsWithFilter.append(h.split('-')[0])\n",
    "    \n",
    "    return hashIdsWithFilter, newsMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc109b3a-de3f-4087-b70d-e37680bf1f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(str(datetime.datetime.today().date()))\n",
    "hdlr = logging.FileHandler(\n",
    "    'vectorize-logs/' + str(datetime.datetime.today().date()) + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aeb39ff-a8ed-4fbc-b9c9-732e31ffccaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n"
     ]
    }
   ],
   "source": [
    "def Log(s, flag=True):\n",
    "    if flag:\n",
    "        logger.info(s)\n",
    "        print(s)\n",
    "\n",
    "Log('----', flag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa66a65-eeb8-4f1b-8ef7-91b4cdd92bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_days = 40\n",
    "st_date = datetime.datetime.now() - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405b985a-8cb0-41c1-9c1d-2420c5d7fd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "NIS_RAW_DATA_BASE_PATH = \"gs://inshorts-segment-raw/data/segment-raw-v5/\"\n",
    "\n",
    "path = get_raw_path(datetime.datetime.now().strftime(\"%Y/%m/%d\"))\n",
    "today_data = process_raw_data(path)\n",
    "today_data = today_data.filter(today_data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66a9d30-164f-4931-b3f6-63b835243fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId'))\\\n",
    "        .groupby('deviceId', 'hashId') \\\n",
    "        .agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84825b01-bb44-4e3d-9bd9-39c44ceb6bae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, hashId: string, overallTimeSpent: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.union(today_data)\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f13553-252f-48cc-9d2d-9f76e3539ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing news mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def popularity(data, newsMeanMap):\n",
    "    Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data, newsMeanMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd948653-67ee-4e9e-a38a-40083a899af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/anunayarunav/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anunayarunav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/anunayarunav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|████████████████████████████████████████████| 23281/23281 [00:32<00:00, 726.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = tokenization(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stemming(tokens)\n",
    "    tokens = lemmatizer(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "newsMapProcessed = {}\n",
    "for hId in tqdm(newsMap):\n",
    "    h = hId.split('-')[0]\n",
    "    newsMapProcessed[h] = {}\n",
    "    newsMapProcessed[h]['title'] = preprocess(newsMap[hId]['title'])\n",
    "    newsMapProcessed[h]['content'] = preprocess(newsMap[hId]['content'])\n",
    "    \n",
    "    newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] + newsMapProcessed[h]['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82689034-cd61-4ca2-809b-c3ff7130dc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import time\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "def newsToTokens(hashId, overallTimeSpent):\n",
    "    tokens = newsMapProcessed[hashId]['features']\n",
    "    #count = int(min(100, overallTimeSpent)/8) + 1\n",
    "    return tokens\n",
    "\n",
    "def toTokenCollection(tokensList):\n",
    "    arr = []\n",
    "    for tokens in tokensList:\n",
    "        for token in tokens:\n",
    "            arr.append(token)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "newsToTokensUdf = F.udf(newsToTokens, ArrayType(StringType()))\n",
    "\n",
    "data_filtered = data.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "data_text = data_filtered.select('deviceId', newsToTokensUdf('hashId', 'overallTimeSpent').alias('tokens')).groupby('deviceId').agg(F.collect_list('tokens').alias('tokensAll'))\n",
    "data_title = data_text.select('deviceId', F.udf(toTokenCollection, ArrayType(StringType()))('tokensAll').alias('tokensList'))\n",
    "row = data_filtered.take(1)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.setInputCol('tokensList')\n",
    "cv.setOutputCol(\"vectors\")\n",
    "\n",
    "model = cv.fit(data_title)\n",
    "\n",
    "data_vector = model.transform(data_title)\n",
    "\n",
    "numClusters = 10\n",
    "lda = LDA(k=numClusters, seed=1, optimizer=\"online\")\n",
    "lda.setFeaturesCol(\"vectors\")\n",
    "\n",
    "ldaModel = lda.fit(data_vector)\n",
    "\n",
    "data_lda = ldaModel.transform(data_vector)\n",
    "\n",
    "topicDf = data_lda.select('deviceId', 'topicDistribution').toPandas()\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for v in tqdm(topicDf.values):\n",
    "    labels[v[0]] = int(np.argmax(np.array(v[1])))\n",
    "\n",
    "Log(\"Training took %s minutes\"%(int((time.time() - training_start) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65bb4a94-c996-49af-88bd-3f8d2e5339e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3024556/3024556 [00:27<00:00, 110831.82it/s]\n",
      "  6%|██▎                                     | 178829/3024556 [02:23<26:49, 1768.59it/s]22/03/09 09:21:42 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /171.16.11.158:52640\n",
      "java.io.IOException: Connection timed out\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "100%|███████████████████████████████████████| 3024556/3024556 [09:15<00:00, 5448.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from ColabUtils import *\n",
    "\n",
    "deviceVectors = {}\n",
    "for v in tqdm(topicDf.values):\n",
    "    deviceVectors[v[0]] = [float(i) for i in v[1]]\n",
    "insertDeviceVectorsInMongo(deviceVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93038da1-b66d-4f48-af40-f509ba3db47b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3024556/3024556 [00:01<00:00, 1513935.48it/s]\n"
     ]
    }
   ],
   "source": [
    "newClusterWiseDevices = defaultdict(lambda: set())\n",
    "for deviceId, cluster in tqdm(labels.items()):\n",
    "    newClusterWiseDevices[cluster].add(deviceId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b37a96-54a8-43f3-a38e-019bfe618235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35274b0c-4d9a-4e14-8679-0985ea9a3a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ColabUtils import *\n",
    "deviceClusterMapExisiting = getDeviceClustersFromMongo([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e081ff2c-c993-48bf-b94f-97a53ac44833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13012497"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd1d96b-f0dd-42aa-b212-803463657a23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 6868334/6868334 [00:06<00:00, 982225.95it/s]\n"
     ]
    }
   ],
   "source": [
    "clusterWiseDevices = defaultdict(lambda: set())\n",
    "for d in tqdm(deviceClusterMapExisiting):\n",
    "    clusterWiseDevices[deviceClusterMapExisiting[d]].add(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b060b64-1f3d-4b02-b9dd-0399aac072fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [len(newClusterWiseDevices[x]) for x in newClusterWiseDevices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a82bd82-2430-436b-a192-c556c4ea651e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping cluster 5 to 4, %age intersection = 10.925\n",
      "Mapping cluster 4 to 5, %age intersection = 30.256\n",
      "Mapping cluster 7 to 2, %age intersection = 18.144\n",
      "Mapping cluster 3 to 0, %age intersection = 23.84\n",
      "Mapping cluster 1 to 1, %age intersection = 14.43\n",
      "Mapping cluster 9 to 7, %age intersection = 2.171\n",
      "Mapping cluster 0 to 6, %age intersection = 13.46\n",
      "Mapping cluster 2 to 8, %age intersection = 9.854\n",
      "Mapping cluster 6 to 9, %age intersection = 0.596\n",
      "Mapping cluster 8 to 3, %age intersection = 6.181\n"
     ]
    }
   ],
   "source": [
    "marked = {}\n",
    "clusterMap = {}\n",
    "for x in newClusterWiseDevices:\n",
    "    mxinter = -1\n",
    "    mxidx = -1\n",
    "    for y in clusterWiseDevices:\n",
    "        if y in marked:\n",
    "            continue\n",
    "        inter = round(len(clusterWiseDevices[y].intersection(newClusterWiseDevices[x])) / len(clusterWiseDevices[y])*100, 3)\n",
    "        if inter > mxinter:\n",
    "            mxinter = inter\n",
    "            mxidx = y\n",
    "            \n",
    "    if mxidx < 0:\n",
    "        mxidx = x\n",
    "    \n",
    "    marked[mxidx] = 1\n",
    "    clusterMap[x] = mxidx\n",
    "    Log(\"Mapping cluster \" + str(x) + \" to \" + str(mxidx) + \", %age intersection = \" + str(mxinter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05631f8f-0a45-41f7-9345-7a413f346f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 297414/297414 [00:00<00:00, 1201545.68it/s]\n",
      "100%|██████████████████████████████████████| 374325/374325 [00:00<00:00, 1105517.58it/s]\n",
      "100%|███████████████████████████████████████| 221136/221136 [00:00<00:00, 919681.44it/s]\n",
      "100%|██████████████████████████████████████| 423204/423204 [00:00<00:00, 1128699.00it/s]\n",
      "100%|███████████████████████████████████████| 319002/319002 [00:00<00:00, 802755.64it/s]\n",
      "100%|██████████████████████████████████████| 327666/327666 [00:00<00:00, 1136075.36it/s]\n",
      "100%|██████████████████████████████████████| 134277/134277 [00:00<00:00, 1030681.82it/s]\n",
      "100%|██████████████████████████████████████| 382850/382850 [00:00<00:00, 1066925.67it/s]\n",
      "100%|███████████████████████████████████████| 417738/417738 [00:00<00:00, 703870.49it/s]\n",
      "100%|██████████████████████████████████████| 126944/126944 [00:00<00:00, 1031048.56it/s]\n"
     ]
    }
   ],
   "source": [
    "updatedDeviceClusters = {}\n",
    "for cluster in newClusterWiseDevices:\n",
    "    for deviceId in tqdm(newClusterWiseDevices[cluster]):\n",
    "        updatedDeviceClusters[deviceId] = clusterMap[cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a55d2abe-0a32-4de2-b17f-a4b1973f09f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100000 Device Clusters\n",
      "Inserted 200000 Device Clusters\n",
      "Inserted 300000 Device Clusters\n",
      "Inserted 400000 Device Clusters\n",
      "Inserted 500000 Device Clusters\n",
      "Inserted 600000 Device Clusters\n",
      "Inserted 700000 Device Clusters\n",
      "Inserted 800000 Device Clusters\n",
      "Inserted 900000 Device Clusters\n",
      "Inserted 1000000 Device Clusters\n",
      "Inserted 1100000 Device Clusters\n",
      "Inserted 1200000 Device Clusters\n",
      "Inserted 1300000 Device Clusters\n",
      "Inserted 1400000 Device Clusters\n",
      "Inserted 1500000 Device Clusters\n",
      "Inserted 1600000 Device Clusters\n",
      "Inserted 1700000 Device Clusters\n",
      "Inserted 1800000 Device Clusters\n",
      "Inserted 1900000 Device Clusters\n",
      "Inserted 2000000 Device Clusters\n",
      "Inserted 2100000 Device Clusters\n",
      "Updated 71.1 device clusters\n"
     ]
    }
   ],
   "source": [
    "insert_start = time.time()\n",
    "from ColabUtils import *\n",
    "insertDeviceClustersInMongo(updatedDeviceClusters, deviceClusterMapExisiting, Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f46bf1ad-71d2-46e3-b6f1-afaaf10c2c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion took 8.455 percent of time\n"
     ]
    }
   ],
   "source": [
    "insert_time = (time.time() - insert_start)\n",
    "\n",
    "Log(\"Insertion took %s percent of time\" % round(100 * insert_time / (time.time() - start_time), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce22e7e-5509-4e41-85ce-048a9e7bbc29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 09:33:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1058.0 KiB\n",
      "22/03/09 09:34:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1056.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataWithClusters = data_filtered.select('hashId', 'overallTimeSpent', F.udf(lambda deviceId: updatedDeviceClusters[deviceId], IntegerType())('deviceId').alias('cluster'))\n",
    "newsClusterMean = dataWithClusters.groupby('hashId', 'cluster').agg(F.sum('overallTimeSpent'),F.count('hashId'))\n",
    "newsClusterMeanDf = newsClusterMean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02704b5b-0b85-47e7-997d-4ec85348e15f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 125570/125570 [00:00<00:00, 567297.85it/s]\n"
     ]
    }
   ],
   "source": [
    "newsTSpentMap = defaultdict(lambda: defaultdict(lambda: (7, 1)))\n",
    "for v in tqdm(newsClusterMeanDf.values):\n",
    "    newsTSpentMap[v[0]][v[1]] = (v[2], v[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79fd094-a513-45a7-9ce1-0164250e9e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting News Score Data\n",
      "Inserting News Score took 0.946 percent of time\n"
     ]
    }
   ],
   "source": [
    "Log(\"Inserting News Score Data\")\n",
    "insert_start = time.time()\n",
    "insertNewsTSpentInMongo(newsTSpentMap)\n",
    "insertNewsScoreInMongo(newsTSpentMap)\n",
    "insert_time = (time.time() - insert_start)\n",
    "Log(\"Inserting News Score took %s percent of time\" % round(100 * insert_time / (time.time() - start_time), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bda16b4-fb40-406e-b77e-d28593811a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exectution finished successfully, in : 54 minutes, 20 seconds \n"
     ]
    }
   ],
   "source": [
    "Log(\"Exectution finished successfully, in : %s minutes, %s seconds \" % (int((time.time() - start_time) / 60), int((time.time() - start_time) % 60)), flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9bf86-c292-476f-a88c-414248b64196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
